# OptiSchema Slim - Environment Variables

# --- LLM Configuration ---
# Options: ollama, openai, gemini
LLM_PROVIDER=ollama

# Ollama (Local)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3

# OpenAI (Cloud)
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o

# Gemini (Cloud)
GEMINI_API_KEY=...

# --- Backend Configuration ---
# Host and Port
BACKEND_HOST=0.0.0.0
BACKEND_PORT=8080
# Enable debug mode
DEBUG=true
# Log level (INFO, DEBUG, WARNING, ERROR)
LOG_LEVEL=INFO

# --- Frontend Configuration ---
# URL of the backend API
NEXT_PUBLIC_API_URL=http://localhost:8080
