#!/usr/bin/env python3
"""
Generate real AI recommendations using the sandbox database.
This connects to the sandbox DB, runs sample queries to generate metrics, then uses AI to analyze them.
"""

import asyncio
import os
import logging
import asyncpg
from datetime import datetime
from analysis.llm import generate_recommendation
from recommendations_db import RecommendationsDB
from models import AnalysisResult
import hashlib
import uuid

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def connect_to_sandbox():
    """Connect to the sandbox database."""
    try:
        # Use Docker service name for connection from within container
        conn = await asyncpg.connect(
            host="postgres_sandbox",  # Docker service name
            port=5432,               # Internal port, not mapped port
            database="sandbox", 
            user="sandbox",          # Correct username
            password="sandbox_pass"  # Correct password
        )
        return conn
    except Exception as e:
        print(f"âŒ Failed to connect to sandbox database: {e}")
        print("ğŸ’¡ Trying alternative connection...")
        try:
            # Fallback: try with localhost (if running outside Docker)
            conn = await asyncpg.connect(
                host="localhost",
                port=5433,  # Mapped port
                database="sandbox", 
                user="sandbox",          # Correct username
                password="sandbox_pass"  # Correct password
            )
            return conn
        except Exception as e2:
            print(f"âŒ Alternative connection also failed: {e2}")
            return None

async def run_sample_queries(conn):
    """Run sample queries to generate realistic performance data."""
    
    print("ğŸ“Š Running sample queries to generate performance data...")
    
    # Sample queries that would realistically need optimization
    sample_queries = [
        {
            "sql": "SELECT * FROM users WHERE email LIKE '%@company.com' ORDER BY created_at DESC LIMIT 10;",
            "description": "User lookup by email domain with sorting"
        },
        {
            "sql": "SELECT u.username, COUNT(o.id) as order_count FROM users u LEFT JOIN orders o ON u.id = o.user_id WHERE u.created_at > '2024-01-01' GROUP BY u.id, u.username ORDER BY order_count DESC;",
            "description": "User order statistics with date filtering"
        },
        {
            "sql": "SELECT p.name, p.price, COUNT(oi.id) as times_ordered FROM products p LEFT JOIN order_items oi ON p.id = oi.product_id WHERE p.category = 'Electronics' GROUP BY p.id ORDER BY times_ordered DESC;",
            "description": "Product popularity analysis by category"
        },
        {
            "sql": "SELECT o.*, u.username FROM orders o JOIN users u ON o.user_id = u.id WHERE o.status = 'pending' AND o.created_at < NOW() - INTERVAL '24 hours';",
            "description": "Stale pending orders analysis"
        }
    ]
    
    query_metrics = []
    
    for i, query_info in enumerate(sample_queries):
        try:
            sql = query_info["sql"]
            description = query_info["description"]
            
            print(f"   Running query {i+1}: {description}")
            
            # Get execution plan and timing
            explain_sql = f"EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) {sql}"
            
            start_time = datetime.now()
            result = await conn.fetch(explain_sql)
            end_time = datetime.now()
            
            execution_time = (end_time - start_time).total_seconds() * 1000  # Convert to ms
            
            # Parse the explain result - handle different response formats
            if result and len(result) > 0:
                explain_result = result[0][0]  # Get the JSON from the first row, first column
                
                # Handle case where result is already parsed JSON
                if isinstance(explain_result, str):
                    import json
                    explain_data = json.loads(explain_result)
                else:
                    explain_data = explain_result
                    
                # Extract plan data
                if isinstance(explain_data, list) and len(explain_data) > 0:
                    plan_data = explain_data[0]
                else:
                    plan_data = explain_data
                    
                plan_info = plan_data.get('Plan', {}) if isinstance(plan_data, dict) else {}
                planning_time = plan_data.get('Planning Time', 0.1) if isinstance(plan_data, dict) else 0.1
                execution_time_from_plan = plan_data.get('Execution Time', execution_time) if isinstance(plan_data, dict) else execution_time
            else:
                explain_data = {}
                plan_info = {}
                planning_time = 0.1
                execution_time_from_plan = execution_time
            
            # Create query hash
            query_hash = hashlib.md5(sql.encode()).hexdigest()[:16]
            
            # Simulate realistic metrics based on query complexity
            calls = 50 + (i * 25)  # Simulate different call frequencies
            total_time = execution_time_from_plan * calls
            
            metrics = {
                'query_hash': query_hash,
                'query_text': sql,
                'calls': calls,
                'total_time': total_time,
                'mean_time': execution_time,
                'execution_time': execution_time,
                'planning_time': planning_time,
                'explain_plan': explain_data,
                'description': description,
                'rows': plan_info.get('Actual Rows', 0),
                'startup_cost': plan_info.get('Startup Cost', 0),
                'total_cost': plan_info.get('Total Cost', 0)
            }
            
            query_metrics.append(metrics)
            
        except Exception as e:
            print(f"   âš ï¸  Query {i+1} failed: {e}")
            continue
    
    print(f"âœ… Generated metrics for {len(query_metrics)} queries")
    return query_metrics

async def analyze_with_ai(query_metrics):
    """Use AI to analyze query metrics and generate recommendations."""
    
    print("ğŸ¤– Analyzing queries with AI to generate recommendations...")
    
    # Expect API keys from environment; select provider if configured
    if not os.getenv('LLM_PROVIDER'):
        os.environ['LLM_PROVIDER'] = 'gemini'
    
    recommendations = []
    
    for i, metrics in enumerate(query_metrics):
        try:
            print(f"   ğŸ§  AI analyzing query {i+1}: {metrics['description']}")
            
            # Prepare data for AI analysis
            query_data = {
                "query_text": metrics['query_text'],
                "execution_time": metrics['execution_time'],
                "calls": metrics['calls'], 
                "total_time": metrics['total_time'],
                "explain_plan": metrics['explain_plan'],
                "actual_metrics": {
                    "execution_time_ms": metrics['execution_time'],
                    "calls": metrics['calls'],
                    "total_time_ms": metrics['total_time'],
                    "planning_time_ms": metrics['planning_time'],
                    "rows_returned": metrics['rows']
                },
                "bottleneck_type": "performance_analysis",
                "performance_score": max(10, 100 - (metrics['execution_time'] * 2))  # Simple scoring
            }
            
            # Generate AI recommendation
            ai_rec = await generate_recommendation(query_data)
            
            # Create recommendation record
            recommendation = {
                'id': str(uuid.uuid4()),
                'query_hash': metrics['query_hash'],
                'recommendation_type': 'ai',
                'title': ai_rec.get('title', 'AI Performance Recommendation'),
                'description': ai_rec.get('description', 'AI-generated optimization recommendation'),
                'sql_fix': ai_rec.get('sql_fix'),
                'estimated_improvement_percent': int(ai_rec.get('estimated_improvement', '0').rstrip('%')) if ai_rec.get('estimated_improvement') != 'Unknown' else 15,
                'confidence_score': ai_rec.get('confidence', 75),
                'risk_level': ai_rec.get('risk_level', 'medium').lower(),
                'status': 'pending',
                'applied': False,
                'created_at': datetime.utcnow().isoformat()
            }
            
            recommendations.append(recommendation)
            
            # Show what was generated
            has_sql = 'ğŸ”§ Executable' if recommendation['sql_fix'] else 'ğŸ“‹ Advisory'
            print(f"   âœ… {has_sql}: {recommendation['title']}")
            if recommendation['sql_fix']:
                sql_preview = recommendation['sql_fix'][:60] + "..." if len(recommendation['sql_fix']) > 60 else recommendation['sql_fix']
                print(f"      SQL: {sql_preview}")
            
        except Exception as e:
            print(f"   âŒ AI analysis failed for query {i+1}: {e}")
            continue
    
    return recommendations

async def generate_demo_recommendations():
    """Main function to generate demo AI recommendations."""
    
    print("ğŸš€ Generating REAL AI Recommendations from Sandbox Database")
    print("=" * 60)
    
    # Connect to sandbox database
    conn = await connect_to_sandbox()
    if not conn:
        return 0
    
    try:
        # Run sample queries to generate metrics
        query_metrics = await run_sample_queries(conn)
        
        if not query_metrics:
            print("âŒ No query metrics generated")
            return 0
        
        # Analyze with AI
        recommendations = await analyze_with_ai(query_metrics)
        
        if not recommendations:
            print("âŒ No AI recommendations generated")
            return 0
        
        # Store recommendations
        print(f"\nğŸ’¾ Storing {len(recommendations)} AI recommendations...")
        stored_count = 0
        
        for rec in recommendations:
            try:
                rec_id = RecommendationsDB.store_recommendation(rec)
                stored_count += 1
            except Exception as e:
                print(f"âŒ Failed to store recommendation: {e}")
        
        print(f"\nğŸ‰ SUCCESS!")
        print(f"Generated and stored {stored_count} REAL AI recommendations")
        print("âœ… These are based on actual query analysis and AI insights")
        
        return stored_count
        
    finally:
        await conn.close()

if __name__ == "__main__":
    print("ğŸ¤– Demo AI Recommendation Generator")
    print("Uses sandbox database + real AI analysis")
    print()
    
    count = asyncio.run(generate_demo_recommendations())
    
    if count > 0:
        print(f"\nğŸŒ Refresh your frontend to see {count} real AI recommendations!")
        print("ğŸ¯ These are dynamically generated, not hardcoded!")
    else:
        print("\nâŒ Failed to generate recommendations")
        print("ğŸ’¡ Check sandbox database connection") 